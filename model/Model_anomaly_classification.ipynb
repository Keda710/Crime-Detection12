{"cells":[{"cell_type":"markdown","metadata":{},"source":["# <center>Anomaly Detection</center>\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T16:42:51.456883Z","iopub.status.busy":"2024-02-18T16:42:51.456073Z","iopub.status.idle":"2024-02-18T17:07:39.050654Z","shell.execute_reply":"2024-02-18T17:07:39.049650Z","shell.execute_reply.started":"2024-02-18T16:42:51.456849Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","Loading Fighting: 100%|██████████| 24684/24684 [03:27<00:00, 118.82it/s]\n","Loading Shoplifting: 100%|██████████| 24835/24835 [03:29<00:00, 118.82it/s]\n","Loading Abuse: 100%|██████████| 19076/19076 [02:36<00:00, 122.26it/s]\n","Loading Arrest: 100%|██████████| 26397/26397 [03:18<00:00, 133.11it/s]\n","Loading Robbery: 100%|██████████| 41493/41493 [05:09<00:00, 133.95it/s]\n","Loading Explosion: 100%|██████████| 18753/18753 [02:01<00:00, 154.00it/s]\n","Loading Fighting: 100%|██████████| 1231/1231 [00:08<00:00, 150.53it/s]\n","Loading Shoplifting: 100%|██████████| 7623/7623 [00:57<00:00, 131.56it/s]\n","Loading Abuse: 100%|██████████| 297/297 [00:01<00:00, 172.27it/s]\n","Loading Arrest: 100%|██████████| 3365/3365 [00:24<00:00, 138.20it/s]\n","Loading Shooting: 100%|██████████| 7630/7630 [00:58<00:00, 131.10it/s]\n","Loading Robbery: 100%|██████████| 835/835 [00:05<00:00, 146.67it/s]\n","Loading Explosion: 100%|██████████| 6510/6510 [01:02<00:00, 103.56it/s]"]},{"name":"stdout","output_type":"stream","text":["Loaded 189869 images.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from keras.preprocessing.image import ImageDataGenerator\n","import os\n","import cv2\n","import pickle\n","from tqdm import tqdm\n","import numpy as np\n","import random\n","\n","test_dir = '/kaggle/input/ucf-crime-dataset/Test'\n","train_dir = '/kaggle/input/ucf-crime-dataset/Train'\n","\n","# Define the categories and labels\n","categories_labels = {'Fighting': 0, 'Shoplifting': 1, 'Abuse': 2, 'Arrest': 3, 'Shooting': 4, 'Robbery': 5, 'Explosion': 6}\n","\n","def load_data(base_dir, categories_labels):\n","    data = []\n","    \n","    # Go through each category\n","    for category, label in categories_labels.items():\n","        # The path to the category directory\n","        category_dir = os.path.join(base_dir, category)\n","\n","        # Make sure the directory exists\n","        if os.path.isdir(category_dir):\n","            # Go through each file in the directory\n","            for filename in tqdm(os.listdir(category_dir), desc=f\"Loading {category}\"):\n","                # Make sure the file is an image\n","                if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n","                    # The path to the image\n","                    image_path = os.path.join(category_dir, filename)\n","\n","                    try:\n","                        # Load the image\n","                        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","                        # Resize the image\n","                        image = cv2.resize(image, (50, 50))\n","\n","                        # Reshape the image to 4D array (ImageDataGenerator requires 4D array)\n","                        image = image.reshape((1,) + image.shape + (1,))\n","\n","                        # Add the image and its label to the data\n","                        data.append([image, label])\n","                    except Exception as e:\n","                        print(f\"Error loading image {image_path}: {e}\")\n","\n","    return data\n","\n","# Load the training and test data\n","training_data = load_data(train_dir, categories_labels)\n","test_data = load_data(test_dir, categories_labels)\n","\n","# Combine the training and test data\n","total_data = training_data + test_data\n","\n","print(f\"Loaded {len(total_data)} images.\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T17:13:07.113761Z","iopub.status.busy":"2024-02-18T17:13:07.112788Z","iopub.status.idle":"2024-02-18T17:58:45.665299Z","shell.execute_reply":"2024-02-18T17:58:45.664172Z","shell.execute_reply.started":"2024-02-18T17:13:07.113726Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(189869, 1, 50, 50, 1)\n","Epoch 1/20\n"]},{"name":"stderr","output_type":"stream","text":["2024-02-18 17:13:17.087996: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"]},{"name":"stdout","output_type":"stream","text":["171/171 [==============================] - ETA: 0s - loss: 1.6680 - accuracy: 0.5675\n","Epoch 1: val_loss improved from inf to 2.39159, saving model to CNN_LSTM.h5\n","171/171 [==============================] - 149s 805ms/step - loss: 1.6680 - accuracy: 0.5675 - val_loss: 2.3916 - val_accuracy: 0.5389\n","Epoch 2/20\n","171/171 [==============================] - ETA: 0s - loss: 0.3095 - accuracy: 0.9048\n","Epoch 2: val_loss improved from 2.39159 to 0.67977, saving model to CNN_LSTM.h5\n","171/171 [==============================] - 136s 795ms/step - loss: 0.3095 - accuracy: 0.9048 - val_loss: 0.6798 - val_accuracy: 0.8564\n","Epoch 3/20\n","171/171 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9649\n","Epoch 3: val_loss improved from 0.67977 to 0.27944, saving model to CNN_LSTM.h5\n","171/171 [==============================] - 136s 796ms/step - loss: 0.1198 - accuracy: 0.9649 - val_loss: 0.2794 - val_accuracy: 0.9282\n","Epoch 4/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9800\n","Epoch 4: val_loss improved from 0.27944 to 0.08282, saving model to CNN_LSTM.h5\n","171/171 [==============================] - 136s 795ms/step - loss: 0.0686 - accuracy: 0.9800 - val_loss: 0.0828 - val_accuracy: 0.9752\n","Epoch 5/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9890\n","Epoch 6: val_loss improved from 0.08282 to 0.01767, saving model to CNN_LSTM.h5\n","171/171 [==============================] - 136s 795ms/step - loss: 0.0369 - accuracy: 0.9890 - val_loss: 0.0177 - val_accuracy: 0.9948\n","Epoch 7/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9906\n","Epoch 7: val_loss did not improve from 0.01767\n","171/171 [==============================] - 136s 794ms/step - loss: 0.0301 - accuracy: 0.9906 - val_loss: 0.0281 - val_accuracy: 0.9912\n","Epoch 8/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9916\n","Epoch 8: val_loss improved from 0.01767 to 0.01595, saving model to CNN_LSTM.h5\n","171/171 [==============================] - 136s 796ms/step - loss: 0.0269 - accuracy: 0.9916 - val_loss: 0.0159 - val_accuracy: 0.9952\n","Epoch 9/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9926\n","Epoch 9: val_loss improved from 0.01595 to 0.01445, saving model to CNN_LSTM.h5\n","171/171 [==============================] - 136s 796ms/step - loss: 0.0229 - accuracy: 0.9926 - val_loss: 0.0144 - val_accuracy: 0.9952\n","Epoch 10/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9930\n","Epoch 10: val_loss did not improve from 0.01445\n","171/171 [==============================] - 136s 795ms/step - loss: 0.0215 - accuracy: 0.9930 - val_loss: 0.0145 - val_accuracy: 0.9952\n","Epoch 11/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9939\n","Epoch 11: val_loss improved from 0.01445 to 0.01366, saving model to CNN_LSTM.h5\n","171/171 [==============================] - 136s 795ms/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.0137 - val_accuracy: 0.9956\n","Epoch 12/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9943\n","Epoch 12: val_loss improved from 0.01366 to 0.01185, saving model to CNN_LSTM.h5\n","171/171 [==============================] - 136s 796ms/step - loss: 0.0176 - accuracy: 0.9943 - val_loss: 0.0119 - val_accuracy: 0.9963\n","Epoch 13/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9948\n","Epoch 14: val_loss did not improve from 0.01044\n","171/171 [==============================] - 136s 795ms/step - loss: 0.0159 - accuracy: 0.9948 - val_loss: 0.0113 - val_accuracy: 0.9967\n","Epoch 15/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9953\n","Epoch 15: val_loss did not improve from 0.01044\n","171/171 [==============================] - 136s 794ms/step - loss: 0.0139 - accuracy: 0.9953 - val_loss: 0.0124 - val_accuracy: 0.9964\n","Epoch 16/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9953\n","Epoch 16: val_loss did not improve from 0.01044\n","171/171 [==============================] - 136s 794ms/step - loss: 0.0139 - accuracy: 0.9953 - val_loss: 0.0109 - val_accuracy: 0.9966\n","Epoch 17/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9951\n","Epoch 17: val_loss did not improve from 0.01044\n","171/171 [==============================] - 136s 794ms/step - loss: 0.0143 - accuracy: 0.9951 - val_loss: 0.0127 - val_accuracy: 0.9965\n","Epoch 18/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9954\n","Epoch 18: val_loss did not improve from 0.01044\n","171/171 [==============================] - 136s 794ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0112 - val_accuracy: 0.9964\n","Epoch 19/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9957\n","Epoch 19: val_loss did not improve from 0.01044\n","171/171 [==============================] - 136s 795ms/step - loss: 0.0131 - accuracy: 0.9957 - val_loss: 0.0107 - val_accuracy: 0.9969\n","Epoch 20/20\n","171/171 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9956\n","Epoch 20: val_loss improved from 0.01044 to 0.00980, saving model to CNN_LSTM.h5\n","171/171 [==============================] - 136s 795ms/step - loss: 0.0137 - accuracy: 0.9956 - val_loss: 0.0098 - val_accuracy: 0.9968\n","('Training time=', 2733.7437813282013)\n"]}],"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from keras.utils import np_utils\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, LeakyReLU\n","from keras.layers import LSTM, TimeDistributed, Conv1D, MaxPooling1D\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import concatenate\n","from keras.utils.vis_utils import plot_model\n","from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n","import time\n","\n","\n","# Initialize lists to store the images and the labels\n","images = []\n","labels = []\n","\n","# Go through each image and its label in the total_data\n","for image, label in total_data:\n","    images.append(image)\n","    labels.append(label)\n","\n","# Convert the lists into numpy arrays\n","images = np.array(images)\n","labels = np.array(labels)\n","print(images.shape)\n","\n","# Reshape images for LSTM\n","images_lstm = images.reshape(images.shape[0], -1, 1)  # Added third dimension for features\n","\n","# Set a seed for reproducibility\n","seed = 42\n","\n","# Split the data into training and testing sets for CNN\n","train_images_cnn, test_images_cnn, train_labels_cnn, test_labels_cnn = train_test_split(images, labels, test_size=0.1, random_state=seed)\n","\n","# Split the data into training and testing sets for LSTM\n","train_images_lstm, test_images_lstm, train_labels_lstm, test_labels_lstm = train_test_split(images_lstm, labels, test_size=0.1, random_state=seed)\n","\n","# Convert labels to categorical for CNN\n","train_labels_cnn = np_utils.to_categorical(train_labels_cnn, len(categories_labels))\n","test_labels_cnn = np_utils.to_categorical(test_labels_cnn, len(categories_labels))\n","\n","# Convert labels to categorical for LSTM\n","train_labels_lstm = np_utils.to_categorical(train_labels_lstm, len(categories_labels))\n","test_labels_lstm = np_utils.to_categorical(test_labels_lstm, len(categories_labels))\n","\n","# Remove the second dimension from your data\n","train_images_cnn = np.squeeze(train_images_cnn, axis=1)\n","test_images_cnn = np.squeeze(test_images_cnn, axis=1)\n","\n","\n","# CNN Model\n","model_CNN = Sequential()\n","model_CNN.add(Conv2D(64, kernel_size=(3, 3), padding='same', input_shape=(50, 50, 1)))\n","model_CNN.add(LeakyReLU(alpha=0.1)) \n","model_CNN.add(MaxPooling2D((2, 2), padding='same')) \n","model_CNN.add(Dropout(0.25))\n","model_CNN.add(Conv2D(128, (3, 3), padding='same')) \n","model_CNN.add(LeakyReLU(alpha=0.1)) \n","model_CNN.add(MaxPooling2D(pool_size=(2, 2), padding='same')) \n","model_CNN.add(Dropout(0.25))\n","model_CNN.add(Conv2D(256, (3, 3), padding='same'))\n","model_CNN.add(LeakyReLU(alpha=0.1))\n","model_CNN.add(MaxPooling2D(pool_size=(2, 2), padding='same')) \n","model_CNN.add(Dropout(0.4))  \n","model_CNN.add(Flatten()) \n","model_CNN.add(Dense(256)) \n","model_CNN.add(LeakyReLU(alpha=0.1))            \n","model_CNN.add(Dropout(0.5)) \n","\n","# LSTM Model\n","model_lstm = Sequential()\n","model_lstm.add(LSTM(units = 8, return_sequences = True, input_shape = (2500, 1), activation='tanh'))\n","model_lstm.add(LSTM(units = 8, return_sequences = True))\n","model_lstm.add(Dense(4, activation='tanh'))\n","model_lstm.add(Dropout(0.2))\n","model_lstm.add(Flatten())\n","\n","# Combine CNN and LSTM model\n","nb_classes = 7\n","combined = concatenate([model_CNN.output, model_lstm.output], axis=-1)\n","output = Dense(nb_classes, activation='softmax')(combined)\n","model_final = Model(inputs=[model_CNN.input, model_lstm.input], outputs=output)\n","\n","# Plot and compile the model\n","plot_model(model_final, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n","\n","model_final.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Callbacks\n","csv_logger = CSVLogger('training.log', separator=',', append=False)\n","mc = ModelCheckpoint('CNN_LSTM.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n","\n","# Training\n","time1 = time.time()\n","history = model_final.fit([train_images_cnn, train_images_lstm], train_labels_lstm, batch_size=1000, epochs=20, validation_data=([test_images_cnn, test_images_lstm], test_labels_lstm), callbacks=[mc, csv_logger])\n","print ((\"Training time=\", time.time()-time1))\n","\n","# Save training history\n","np.save(\"CNN_LSTM_history.npy\", history.history)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T17:58:45.668165Z","iopub.status.busy":"2024-02-18T17:58:45.667472Z","iopub.status.idle":"2024-02-18T17:59:18.891731Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," conv2d_input (InputLayer)      [(None, 50, 50, 1)]  0           []                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 50, 50, 64)   640         ['conv2d_input[0][0]']           \n","                                                                                                  \n"," leaky_re_lu (LeakyReLU)        (None, 50, 50, 64)   0           ['conv2d[0][0]']                 \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 25, 25, 64)   0           ['leaky_re_lu[0][0]']            \n","                                                                                                  \n"," dropout (Dropout)              (None, 25, 25, 64)   0           ['max_pooling2d[0][0]']          \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 25, 25, 128)  73856       ['dropout[0][0]']                \n","                                                                                                  \n"," leaky_re_lu_1 (LeakyReLU)      (None, 25, 25, 128)  0           ['conv2d_1[0][0]']               \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 13, 13, 128)  0          ['leaky_re_lu_1[0][0]']          \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 13, 13, 128)  0           ['max_pooling2d_1[0][0]']        \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 13, 13, 256)  295168      ['dropout_1[0][0]']              \n","                                                                                                  \n"," leaky_re_lu_2 (LeakyReLU)      (None, 13, 13, 256)  0           ['conv2d_2[0][0]']               \n","                                                                                                  \n"," max_pooling2d_2 (MaxPooling2D)  (None, 7, 7, 256)   0           ['leaky_re_lu_2[0][0]']          \n","                                                                                                  \n"," lstm_input (InputLayer)        [(None, 2500, 1)]    0           []                               \n","                                                                                                  \n"," dropout_2 (Dropout)            (None, 7, 7, 256)    0           ['max_pooling2d_2[0][0]']        \n","                                                                                                  \n"," lstm (LSTM)                    (None, 2500, 8)      320         ['lstm_input[0][0]']             \n","                                                                                                  \n"," flatten (Flatten)              (None, 12544)        0           ['dropout_2[0][0]']              \n","                                                                                                  \n"," lstm_1 (LSTM)                  (None, 2500, 8)      544         ['lstm[0][0]']                   \n","                                                                                                  \n"," dense (Dense)                  (None, 256)          3211520     ['flatten[0][0]']                \n","                                                                                                  \n"," dense_1 (Dense)                (None, 2500, 4)      36          ['lstm_1[0][0]']                 \n","                                                                                                  \n"," leaky_re_lu_3 (LeakyReLU)      (None, 256)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dropout_4 (Dropout)            (None, 2500, 4)      0           ['dense_1[0][0]']                \n","                                                                                                  \n"," dropout_3 (Dropout)            (None, 256)          0           ['leaky_re_lu_3[0][0]']          \n","                                                                                                  \n"," flatten_1 (Flatten)            (None, 10000)        0           ['dropout_4[0][0]']              \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 10256)        0           ['dropout_3[0][0]',              \n","                                                                  'flatten_1[0][0]']              \n","                                                                                                  \n"," dense_2 (Dense)                (None, 7)            71799       ['concatenate[0][0]']            \n","                                                                                                  \n","==================================================================================================\n","Total params: 3,653,883\n","Trainable params: 3,653,883\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","594/594 [==============================] - 32s 53ms/step - loss: 0.0098 - accuracy: 0.9968\n","0.00979856587946415 0.9968399405479431\n"]}],"source":["from keras.models import load_model\n","fashion_model = load_model('./CNN_LSTM.h5') # load model\n","fashion_model.summary() # summarize model.\n","\n","from contextlib import redirect_stdout\n","with open('./CNN_LSTM'+\".xls\", 'w') as f:\n","    with redirect_stdout(f):\n","        fashion_model.summary()\n","        \n","val_loss, val_accuracy=fashion_model.evaluate([test_images_cnn, test_images_lstm] ,test_labels_cnn) ## to get test accuracy and losses\n","print(val_loss, val_accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T17:59:18.925561Z","iopub.status.busy":"2024-02-18T17:59:18.925308Z","iopub.status.idle":"2024-02-18T18:05:12.628055Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["594/594 [==============================] - 32s 53ms/step\n","classification time: 32.727832078933716\n","[[2530    1    0    1    0    0    0]\n"," [   0 3306    0    1    0    2    1]\n"," [   1    0 1960    1    0    0    0]\n"," [   0    3    0 2993    3    2    1]\n"," [   3    0    1   17 1437    1    0]\n"," [   6    0    2    1    0 4121    2]\n"," [   2    0    2    2    0    4 2580]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      2532\n","           1       1.00      1.00      1.00      3310\n","           2       1.00      1.00      1.00      1962\n","           3       0.99      1.00      0.99      3002\n","           4       1.00      0.98      0.99      1459\n","           5       1.00      1.00      1.00      4132\n","           6       1.00      1.00      1.00      2590\n","\n","    accuracy                           1.00     18987\n","   macro avg       1.00      1.00      1.00     18987\n","weighted avg       1.00      1.00      1.00     18987\n","\n","Precision: 0.996846\n","Recall: 0.996840\n","F1 score: 0.996838\n","IoU: 0.9936997952433454\n","594/594 [==============================] - 32s 54ms/step - loss: 0.0098 - accuracy: 0.9968\n","5341/5341 [==============================] - 287s 54ms/step - loss: 0.0068 - accuracy: 0.9978\n","loss_train:  0.006755515467375517 accuracy_train:  0.9978289008140564\n","Test loss: 0.00979856587946415 Test accuracy: 0.9968399405479431\n"]}],"source":["time2=time.time()\n","predict_prob=fashion_model.predict([test_images_cnn, test_images_lstm])\n","y_pred=np.argmax(predict_prob,axis=1)\n","print ('classification time:', time.time()-time2)\n","\n","##print (y_pred)\n","y_true=np.argmax(test_labels_cnn, axis=1)\n","from sklearn.metrics import precision_recall_fscore_support as score, precision_score, recall_score, f1_score\n","from sklearn.metrics import classification_report, confusion_matrix\n","cm = confusion_matrix(y_true, y_pred)\n","print (cm)\n","print(classification_report(y_true, y_pred))\n","\n","precision = precision_score(y_true, y_pred, average='weighted')\n","print('Precision: %f' % precision)\n","# recall: tp / (tp + fn)\n","recall = recall_score(y_true, y_pred, average='weighted')\n","print('Recall: %f' % recall)\n","# f1: tp / (tp + fp + fn)\n","f1 = f1_score(y_true, y_pred, average='weighted')\n","print('F1 score: %f' % f1)\n","#-----------  IoU\n","from sklearn.metrics import jaccard_score\n","print ('IoU:', jaccard_score(y_true, y_pred, average='micro'))\n","\n","\n","test_eval = fashion_model.evaluate([test_images_cnn, test_images_lstm], test_labels_cnn)\n","\n","loss, accuracy = fashion_model.evaluate([train_images_cnn, train_images_lstm], train_labels_cnn)\n","print('loss_train: ', loss, 'accuracy_train: ', accuracy)\n","print('Test loss:', test_eval[0], 'Test accuracy:', test_eval[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-18T18:05:16.434026Z","iopub.status.idle":"2024-02-18T18:05:16.434360Z"},"trusted":true},"outputs":[],"source":["#============================= this code with any image\n","\n","import cv2\n","import numpy as np\n","import os\n","from keras.models import load_model\n","from PIL import Image\n","\n","categories_labels = {'Fighting': 0, 'Shoplifting': 1, 'Abuse': 2, 'Arrest': 3, 'Shooting': 4, 'Robbery': 5, 'Explosion': 6}\n","labels_categories = {v: k for k, v in categories_labels.items()}  # reverse dictionary for label lookup\n","\n","# Load the trained model\n","model = load_model('/kaggle/working/CNN_LSTM.h5')\n","\n","def predict_image(image):\n","    # Resize the image\n","    image = cv2.resize(image, (50, 50))\n","\n","    # Reshape the image to 4D array for CNN and LSTM input\n","    image_cnn = image.reshape((1,) + image.shape + (1,))\n","    image_lstm = image.reshape((1,) + (-1, 1))\n","\n","    # Use the model to predict the category of the image\n","    prediction = model.predict([image_cnn, image_lstm])\n","\n","    # Find the category with the highest probability\n","    label = np.argmax(prediction)\n","\n","    # Return the name of the category\n","    return labels_categories[label]"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1710176,"sourceId":2799594,"sourceType":"datasetVersion"},{"datasetId":3287095,"sourceId":5717154,"sourceType":"datasetVersion"},{"datasetId":3287159,"sourceId":5717242,"sourceType":"datasetVersion"}],"dockerImageVersionId":30476,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
